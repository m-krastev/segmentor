{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import kimimaro\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.plotting import plot_skeleton_3d\n",
    "\n",
    "file = \"../data/nnUNet_raw/Dataset042_small_bowel/labelsTs/s0006.nii.gz\"\n",
    "file = nib.load(file)\n",
    "ground_truth = np.asarray(file.dataobj)\n",
    "print(np.unique(ground_truth))\n",
    "\n",
    "skels = kimimaro.skeletonize(\n",
    "    ground_truth,\n",
    "    teasar_params={\n",
    "        \"scale\": 3,\n",
    "        \"const\": 5,\n",
    "        \"pdrf_scale\": 10000,\n",
    "        \"pdrf_exponent\": 4,\n",
    "        \"soma_acceptance_threshold\": 3500,  # physical units\n",
    "        # \"soma_detection_threshold\": 750,  # physical units\n",
    "        # \"soma_invalidation_const\": 300,  # physical units\n",
    "        # \"soma_invalidation_scale\": 2,\n",
    "    },\n",
    "    anisotropy=(1, 1, 1),\n",
    "    dust_threshold=5,\n",
    "    fix_branching=True,\n",
    "    progress=True,\n",
    "    parallel_chunk_size=100, # for the progress bar\n",
    ")\n",
    "\n",
    "skeleton = skels[1]\n",
    "print(skeleton.cable_length())\n",
    "# skeleton.viewer()\n",
    "\n",
    "# Idea: Discretize the voxel space in coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delaunay = Delaunay(np.asarray(ground_truth.nonzero()).T)\n",
    "print(len(delaunay.simplices))\n",
    "\n",
    "np.concatenate([np.full(delaunay.simplices.shape[0],4).reshape(-1,1), delaunay.simplices], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skely_delauney = Delaunay(skeleton.vertices)\n",
    "\n",
    "skeleton_3d = pv.PolyData(skely_delauney.points, strips=np.concatenate((np.full(skely_delauney.simplices.shape[0],4).reshape(-1,1), skely_delauney.simplices), axis=1).flatten())\n",
    "\n",
    "skeleton_3d.plot(opacity=0.3, show_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(skely_delauney.simplices).shape\n",
    "# Low-energy model that can find the parameters of the twist and turns, the hyperparameters of the tube, and maybe some neural network that can predict the tube from the volume\n",
    "# Generate a Voronoi-like diagram which might find a boundary between the tube and the volume\n",
    "# Superpixel/super-voxel-based segmentation\n",
    "# If we are going into RL, we need to have a reward function that can be examined at each step of the optimization; Martin expresses the opinion that RL should be the last step for refining the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional_points = skeleton.vertices[skely_delauney.simplices].mean(axis=1).astype(int)\n",
    "# print(len(additional_points))\n",
    "\n",
    "# _additional_points = {}\n",
    "# _ignore = {}\n",
    "# for i, point in enumerate(additional_points):\n",
    "#     tup = tuple(point)\n",
    "#     if tup in _additional_points:\n",
    "#         _ignore[i] = i\n",
    "#     else:\n",
    "#         _additional_points[i]=tup\n",
    "# additional_points = np.array(list(_additional_points.values()))\n",
    "\n",
    "# skeleton_2 = skeleton.clone()\n",
    "# skeleton_2.vertices = np.concatenate((skeleton.vertices, additional_points), axis=0)\n",
    "\n",
    "# skely_delauney.simplices \n",
    "\n",
    "# additional_points_idx = np.arange(len(skeleton.vertices), len(skeleton.vertices) + len(additional_points)).reshape(-1,1)\n",
    "\n",
    "# print(skeleton_2.edges.shape)\n",
    "# additional_edges = []\n",
    "# simplices = skely_delauney.simplices[~np.isin(np.arange(len(skely_delauney.simplices)), _ignore)]\n",
    "# for i in range(simplices.shape[1]):\n",
    "#     additional_edges.append(np.concatenate((simplices[:, i].reshape(-1,1), additional_points_idx), axis=1))\n",
    "# additional_edges = np.concatenate(additional_edges, axis=0)\n",
    "\n",
    "# skeleton_2.edges = np.concatenate((skeleton.edges, additional_edges + len(skeleton.vertices)), axis=0)\n",
    "# skeleton_2.radii = np.concatenate((skeleton.radii, np.full(len(additional_edges), 1)), axis=0)\n",
    "# print(len(skeleton_2.edges))\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "# plotter.add_mesh(skeleton_2.vertices, color='red')\n",
    "# plotter.add_mesh(skeleton.vertices, color='blue')\n",
    "# plotter.show()\n",
    "\n",
    "\n",
    "def plot_skeleton_3d(skeleton, volume = None):\n",
    "    \"\"\"Plot a 3D skeleton with pyvista\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skeleton : cloudvolume.Skeleton\n",
    "        The skeleton to plot. Should contain the vertices, edges and radii\n",
    "    volume : np.ndarray, optional\n",
    "        The volume segmentation mask to plot. Default is None.\n",
    "    \"\"\"\n",
    "    plotter = pv.Plotter()\n",
    "\n",
    "    skeleton_3d = pv.PolyData(\n",
    "        skeleton.vertices,\n",
    "        lines=np.concatenate((np.full(skeleton.edges.shape[0], 2).reshape(-1, 1), skeleton.edges), axis=1),\n",
    "    )\n",
    "\n",
    "    if hasattr(skeleton, \"radii\"):\n",
    "        # Not sure why we need to remove one element, but for some reason the polydata is one element less.\n",
    "        skeleton_3d.cell_data[\"width\"] = skeleton.radii[1:]\n",
    "\n",
    "    plotter.add_mesh(skeleton_3d, show_edges=True, line_width=5, scalars=\"width\")\n",
    "\n",
    "    if volume is not None:\n",
    "        plotter.add_volume(volume * 20, cmap=\"viridis\", specular=0.5, specular_power=15)\n",
    "\n",
    "    plotter.view_xz()\n",
    "    plotter.show()\n",
    "\n",
    "plot_skeleton_3d(skeleton, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skels[1]??\n",
    "# Algorithm for post-processing of the skeleton\n",
    "# 1. Add weak edges to the outside points of the skeleton (using Delaunay points)\n",
    "# 1.5. Add weak edges with a neighbourhood of ~3. \n",
    "# 2. Split the nodes with radius > T into two nodes (iteratively, until all nodes have radius < T)\n",
    "# 3. Add connections for the split nodes (from beginning to end, e.g. 1-2-3 -> 1-2, 2-3, 1-4, 4-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import vtk\n",
    "import pyvista as pv\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def nii_2_mesh(filename_nii, filename_stl, label):\n",
    "    \"\"\"\n",
    "    Read a nifti file including a binary map of a segmented organ with label id = label.\n",
    "    Convert it to a smoothed mesh of type stl.\n",
    "\n",
    "    filename_nii     : Input nifti binary map\n",
    "    filename_stl     : Output mesh name in stl format\n",
    "    label            : segmented label id\n",
    "    \n",
    "    Courtesy of: https://github.com/MahsaShk/MeshProcessing/blob/master/nii_2_mesh_conversion.py\n",
    "    \"\"\"\n",
    "\n",
    "    # read the file\n",
    "    reader = vtk.vtkNIFTIImageReader()\n",
    "    reader.SetFileName(filename_nii)\n",
    "    reader.Update()\n",
    "\n",
    "    # apply marching cube surface generation\n",
    "    surf = vtk.vtkDiscreteMarchingCubes()\n",
    "    surf.SetInputConnection(reader.GetOutputPort())\n",
    "    surf.SetValue(0, label)  # use surf.GenerateValues function if more than one contour is available in the file\n",
    "    surf.Update()\n",
    "\n",
    "    # smoothing the mesh\n",
    "    smoother = vtk.vtkWindowedSincPolyDataFilter()\n",
    "    if vtk.VTK_MAJOR_VERSION <= 5:\n",
    "        smoother.SetInput(surf.GetOutput())\n",
    "    else:\n",
    "        smoother.SetInputConnection(surf.GetOutputPort())\n",
    "    smoother.SetNumberOfIterations(30)\n",
    "    smoother.NonManifoldSmoothingOn()\n",
    "    smoother.NormalizeCoordinatesOn()  # The positions can be translated and scaled such that they fit within a range of [-1, 1] prior to the smoothing computation\n",
    "    smoother.GenerateErrorScalarsOn()\n",
    "    smoother.Update()\n",
    "\n",
    "    # save the output\n",
    "    writer = vtk.vtkSTLWriter()\n",
    "    writer.SetInputConnection(smoother.GetOutputPort())\n",
    "    writer.SetFileTypeToASCII()\n",
    "    writer.SetFileName(filename_stl)\n",
    "    writer.Write()\n",
    "\n",
    "\n",
    "filename_ct = \"../data/nnUNet_raw/Dataset042_small_bowel/imagesTr/s0006_0000.nii.gz\"\n",
    "filename_gt = \"../data/nnUNet_raw/Dataset042_small_bowel/labelsTr/s0006.nii.gz\"\n",
    "filename_stl = \"01.stl\"\n",
    "# nii_2_mesh(filename_nii, filename_stl, 1)\n",
    "ground_truth = np.asarray(nib.load(filename_gt).dataobj)\n",
    "\n",
    "def normalize_ct(nii, percentiles: tuple[float, float] = (0.0005, 0.9995), window: tuple[float, float] =(50, 400)):\n",
    "    nii = np.where((nii > np.quantile(nii, percentiles[0])) & (nii < (np.quantile(nii, percentiles[1]))), nii, 0)\n",
    "    window_c, window_w = window\n",
    "    nii = np.where((nii >= window_c - window_w/2) & (nii <= window_c + window_w/2), nii, 0)\n",
    "    # nii = nii / np.linalg.norm(nii, axis=-1, keepdims=True)\n",
    "    return nii\n",
    "\n",
    "def load_and_normalize(filename, percentiles=(0.0005, 0.9995), window=(50, 400)):\n",
    "    nii = nib.load(filename)\n",
    "    nii = np.asarray(nii.dataobj)\n",
    "    nii = normalize_ct(nii, percentiles, window)\n",
    "    return nii\n",
    "\n",
    "def save_nifti(data, filename, other = None):\n",
    "    if other is None:\n",
    "        other = nib.load(filename)\n",
    "    else:\n",
    "        other = nib.load(other)\n",
    "    new_image = nib.Nifti1Image(data, other.affine, other.header)\n",
    "    nib.save(new_image, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion (05/02/2025)\n",
    "What you want to model is the flow of two cells: and the flow of the two cells is defined by the surface of the cells. The flow goes orthogonal to the surface. Therefore you should go into the direction of the path and not orthogonal to the path.\n",
    "We want to quantify the cost of crossing a wall, not just the cost of all pixels even the ones which don't relate.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
